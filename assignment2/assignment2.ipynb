{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2** \n",
    "\n",
    "This assignment requires you to implement image recognition methods. Please understand and use relevant libraries. You are expected to solve both questions.\n",
    "\n",
    "**Data preparation and rules**\n",
    "\n",
    "Please use the images of the MNIST hand-written digits recognition dataset. You may use torchvision.datasets library to obtain the images and splits. You should have 60,000 training images and 10,000 test images. Use test images only to evaluate your model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.astype('float32') / 255.0\n",
    "    return images\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: SIFT-BoVW-SVM [4 points]\n",
    "\n",
    "1. [2 points] Implement the SIFT detector and descriptor. Compute cluster centers for the Bag-of-Visual-Words approach. Represent the images as histograms (of visual words) and train a linear SVM model for 10-way classification.\n",
    "Note 1: You may want to use libraries such as cv2 (OpenCV) and sklearn (Sci-kit learn) for doing this question. https://scikit-learn.org/stable/modules/svm.html#multi-class-classification may be useful for the SVM.\n",
    "Note 2: Seed random numbers for reproducibility (running the notebook again should give you the same results!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunayana/anaconda3/envs/newenvt/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/Users/sunayana/anaconda3/envs/newenvt/lib/python3.8/site-packages/sklearn/base.py:1151: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (150). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 7.78%\n"
     ]
    }
   ],
   "source": [
    "# # Necessary Imports\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# from sklearn import datasets\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# # Load MNIST Data\n",
    "# def load_mnist():\n",
    "#     digits = datasets.load_digits()\n",
    "#     # MNIST images are 8x8 and already in grayscale\n",
    "#     images = np.array(digits.images)\n",
    "#     labels = np.array(digits.target)\n",
    "#     # Reshape the images for SIFT\n",
    "#     images = [cv2.resize(img, (32, 32)) for img in images]\n",
    "#     return images, labels\n",
    "\n",
    "# # Compute SIFT Features\n",
    "# def compute_sift_features(images):\n",
    "#     sift = cv2.SIFT_create()\n",
    "#     descriptors = []\n",
    "#     for img in images:\n",
    "#         kp, desc = sift.detectAndCompute(img.astype(np.uint8), None)\n",
    "#         if desc is not None:\n",
    "#             descriptors.append(desc)\n",
    "#         else:\n",
    "#             descriptors.append(np.zeros((1, sift.descriptorSize())))\n",
    "#     return descriptors\n",
    "\n",
    "# # K-Means Clustering for BoVW\n",
    "# def cluster_descriptors(descriptors, n_clusters=200):\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     all_descriptors = np.vstack(descriptors)\n",
    "#     kmeans.fit(all_descriptors)\n",
    "#     return kmeans\n",
    "\n",
    "# # Convert Images to Histograms of Visual Words\n",
    "# def convert_to_histograms(descriptors, kmeans):\n",
    "#     histograms = []\n",
    "#     for desc in descriptors:\n",
    "#         hist = np.zeros(kmeans.n_clusters)\n",
    "#         if desc is not None:\n",
    "#             predictions = kmeans.predict(desc)\n",
    "#             for p in predictions:\n",
    "#                 hist[p] += 1\n",
    "#         histograms.append(hist)\n",
    "#     return histograms\n",
    "\n",
    "# # Train a Linear SVM Classifier\n",
    "# def train_svm(histograms, labels):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(histograms, labels, test_size=0.2, random_state=42)\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    \n",
    "#     svm = SVC(kernel='linear', probability=True, random_state=42)\n",
    "#     svm.fit(X_train, y_train)\n",
    "#     print(f\"Test accuracy: {svm.score(X_test, y_test) * 100:.2f}%\")\n",
    "#     return svm\n",
    "\n",
    "# # Main function to execute the workflow\n",
    "# def main():\n",
    "#     images, labels = load_mnist()\n",
    "#     descriptors = compute_sift_features(images)\n",
    "#     kmeans = cluster_descriptors(descriptors, n_clusters=150) # You may adjust the number of clusters\n",
    "#     histograms = convert_to_histograms(descriptors, kmeans)\n",
    "#     svm = train_svm(histograms, labels)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.55%\n",
      "Confusion Matrix:\n",
      "[[ 841    6   17    2    2   27   48   16   12    9]\n",
      " [   0 1110    2    2    4    0    5    9    3    0]\n",
      " [  48   23  654   29   23   24   28  154   40    9]\n",
      " [   7    7   74  736   22   97   15   30   15    7]\n",
      " [   4   22   25   13  761   21   25   25   32   54]\n",
      " [  50   14   36   71   25  547   61   43   15   30]\n",
      " [  68   20   26   12   11   46  587   50   13  125]\n",
      " [  10   74  105   21   24   10   33  734   14    3]\n",
      " [  23    5   43   28   45   19   36   13  709   53]\n",
      " [  24   15   11   15   40   44  130   19   35  676]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.astype('float32') / 255.0\n",
    "    return images\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "\n",
    "def calc_features(images, thresh):\n",
    "    sift = cv2.SIFT_create(thresh)\n",
    "    features = []\n",
    "    for img in images:\n",
    "        img = np.uint8(img * 255)  # Convert back to OpenCV usable format\n",
    "        _, des = sift.detectAndCompute(img, None)\n",
    "        if des is not None:\n",
    "            features.append(des)\n",
    "    return np.vstack(features) if features else np.empty((0, 128))  # Assuming SIFT descriptors have a length of 128\n",
    "\n",
    "\n",
    "def perform_kmeans(features, k):\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    _, _, centers = cv2.kmeans(features, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    return centers\n",
    "\n",
    "def bag_of_features(features, centers, k):\n",
    "    vec = np.zeros((1, k), dtype=np.float32)\n",
    "    for i in range(features.shape[0]):\n",
    "        diff = np.linalg.norm(np.tile(features[i], (k, 1)) - centers, axis=1)\n",
    "        idx = np.argmin(diff)\n",
    "        vec[0, idx] += 1\n",
    "    return vec\n",
    "\n",
    "def train_and_evaluate(train_images, train_labels, test_images, test_labels, thresh, k):\n",
    "    features = calc_features(train_images, thresh)\n",
    "    centers = perform_kmeans(features, k)\n",
    "    \n",
    "    def create_feature_vec(img):\n",
    "        des = calc_features([img], thresh)\n",
    "        if des.size > 0:  # Changed from None check to size check\n",
    "            return bag_of_features(des, centers, k).flatten()\n",
    "        else:\n",
    "            return np.zeros((k,))  # Return a zero vector if no features are detected\n",
    "\n",
    "    # Convert training and testing images to feature vectors\n",
    "    train_vec = np.array([create_feature_vec(img) for img in train_images])\n",
    "    test_vec = np.array([create_feature_vec(img) for img in test_images])\n",
    "    \n",
    "    # Train SVM\n",
    "    clf = SVC(kernel='linear', probability=True)\n",
    "    clf.fit(train_vec, train_labels)\n",
    "    \n",
    "    # Evaluate\n",
    "    preds = clf.predict(test_vec)\n",
    "    return accuracy_score(test_labels, preds), confusion_matrix(test_labels, preds)\n",
    "\n",
    "thresh = 10  # SIFT feature threshold\n",
    "k = 150  # Number of clusters for KMeans\n",
    "accuracy, conf_mat = train_and_evaluate(train_images, train_labels, test_images, test_labels, thresh, k)\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [1 point] Keeping everything else constant, plot how classification accuracy changes as you sweep across 6 different values for the number of clusters. Please decide what numbers are meaningful for this question. Explain the trends in classification accuracy that you observe.\n",
    "Note 1: It is recommended to try hyperparameters in logarithmic steps such as 2x or 3x multiples. An example of 2x multiples is: 1, 2, 5, 10, 20, ... An example of 3x multiples is: 1, 3, 10, 30, 100, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [1 point] Show the results for 6 different hyperparameter settings. You may play with the SIFT detector or descriptor and the linear SVM. Keep the number of clusters constant based on the answer to the previous question. Explain the trends in classification accuracy that you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: CNNs and Transformers [6 points]\n",
    "1. [2.5 points] Set up a modular codebase for training a CNN (LeNet) on the task of handwritten digit recognition. You should have clear functional separation between the data (dataset and dataloader), model (nn.Module), and trainer (train/test epoch loops). Implement logging: using Weights & Biases is highly recommended, alternatively, create your own plots using other plotting libraries. Log the training and evaluation losses and accuracies at every epoch, show the plots for at least one training and evaluation run.\n",
    "Note 1: Seed random numbers for reproducibility (running the notebook again should give you the same results!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [1 point] Show the results for 6 different settings of hyperparameters. You may want to change the batch size, learning rate, and optimizer. Explain the trends in classification accuracy that you observe. Which hyperpa- rameters are most important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [0.5 points] Compare the best performing CNN (from above) against the SIFT-BoVW-SVM approach. Explain the differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [0.5 points] How does the performance change if you double the number of convolutional layers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [0.5 points] How does the performance change as you increase the number of training samples: [0.6K, 1.8K, 6K, 18K, 60K]? Explain the trends in classification accuracy that you observe.\n",
    "Note 1: Make sure that all classes are represented equally within different subsets of the training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [1 point] Replace the CNN model with a 2 layer TransformerEncoder. Using a ViT style prediction scheme, evaluate classification accuracy when training with 6K and 60K images. How do the results compare against CNNs? Explain the trends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
